{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "352SgqRe2lpC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import warnings\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xonf_zEp2seI"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, images, labels=None,transforms=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transforms:\n",
        "           image = self.transforms(Image.fromarray(self.images[idx]))\n",
        "        else:\n",
        "            image = torch.FloatTensor(self.images[idx]).permute(2, 0, 1)\n",
        "        if self.labels is not None:\n",
        "            return image, torch.LongTensor([self.labels[idx]])[0]\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "41ONzdZW2yyh"
      },
      "outputs": [],
      "source": [
        "''' class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        return self.fc3(x)\n",
        " '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn\n",
        "                .Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "          out = F.relu(self.bn1(self.conv1(x)))\n",
        "          out = self.bn2(self.conv2(out))\n",
        "          out += self.shortcut(x)\n",
        "          out = F.relu(out)\n",
        "          return out\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32), nn.GELU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(ResBlock(32, 32), ResBlock(32, 32))\n",
        "        self.layer3 = nn.Sequential(ResBlock(32, 64, stride=2), ResBlock(64, 64))\n",
        "        self.layer4 = nn.Sequential(ResBlock(64, 128, stride=2), ResBlock(128, 128))\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0tngP_MO23T2"
      },
      "outputs": [],
      "source": [
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None: return None\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    # Resize the image to a fixed size (e.g., 224x224)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return img  # trả về uint8, đã resize, chưa /255\n",
        "def load_data(csv_path, image_dir, has_labels=True):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    images, labels = [], []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        img = load_image(os.path.join(image_dir, row['image']))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "            if has_labels:\n",
        "                labels.append(row['label'])\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"Đã tải {idx + 1}/{len(df)} ảnh\")\n",
        "\n",
        "    print(f\"Tải thành công {len(images)} ảnh\")\n",
        "    if has_labels:\n",
        "        return np.array(images), np.array(labels)\n",
        "    return np.array(images), df['image'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kb0BzV9L278n"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Thiết bị: {device}\")\n",
        "\n",
        "    # Tải dữ liệu\n",
        "    X_train, y_train = load_data('/content/drive/MyDrive/Content/CUB/train.csv', '/content/drive/MyDrive/Content/ALL_IMAGES')\n",
        "\n",
        "    # Mã hóa nhãn\n",
        "    encoder = LabelEncoder()\n",
        "    y_encoded = encoder.fit_transform(y_train)\n",
        "    num_classes = len(encoder.classes_)\n",
        "    print(f\"Số lớp: {num_classes}\")\n",
        "\n",
        "    # Chia train/val\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "    #Tinh mean/std\n",
        "    mean = np.mean(X_tr, axis=(0,1,2)) / 255.0  # vì X_tr lúc này là uint8\n",
        "    std  = np.std(X_tr,  axis=(0,1,2)) / 255.0\n",
        "    print(f\"Mean: {mean}, Std: {std}\")\n",
        "\n",
        "\n",
        "    #Transform\n",
        "    train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "    ])\n",
        "\n",
        "    val_tf = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = val_tf\n",
        "\n",
        "    # Tạo dataloader\n",
        "    train_loader = DataLoader(ImageDataset(X_tr, y_tr, transforms=train_tf), batch_size=16, shuffle=True,num_workers=2,pin_memory=True)\n",
        "    val_loader = DataLoader(ImageDataset(X_val, y_val, transforms=val_tf), batch_size=16, shuffle=False,num_workers=2,pin_memory=True)\n",
        "\n",
        "    # Khởi tạo mô hình\n",
        "    model = SmallResNet(num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.02)\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "      warmup, total = 5, 50\n",
        "      if epoch < warmup:\n",
        "          return float(epoch + 1)/warmup\n",
        "      import math\n",
        "      t = (epoch - warmup)/(total - warmup)\n",
        "      return 0.5*(1 + math.cos(math.pi*t))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    print(\"Bắt đầu huấn luyện...\")\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # ✅ chỉ 1 lần forward với AMP\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "            # ✅ backward + clip + step với AMP\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, pred = torch.max(output, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (pred == target).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/50, Batch {batch_idx}, '\n",
        "                      f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "\n",
        "        # ✅ step scheduler SAU mỗi epoch\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            # (tuỳ chọn) dùng AMP ở eval để tăng tốc\n",
        "            for data, target in val_loader:\n",
        "                data = data.to(device, non_blocking=True)\n",
        "                target = target.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    output = model(data)\n",
        "                    vloss = criterion(output, target)\n",
        "                val_loss += vloss.item()\n",
        "                _, pred = torch.max(output, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (pred == target).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch+1} - Train Loss: {train_loss/len(train_loader):.4f}, '\n",
        "              f'Train Acc: {100.*correct/total:.2f}%, '\n",
        "              f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
        "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
        "    # Lưu mô hình\n",
        "    torch.save(model.state_dict(), 'cnn_model_pytorch.pth')\n",
        "\n",
        "    # Dự đoán\n",
        "    X_test, names = load_data('/content/drive/MyDrive/Content/CUB/test.csv', '/content/drive/MyDrive/Content/ALL_IMAGES', has_labels=False)\n",
        "    test_loader = DataLoader(ImageDataset(X_test,transforms=test_tf), batch_size=16, shuffle=False,num_workers=2)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "\n",
        "    # Lưu kết quả\n",
        "    pred_labels = encoder.inverse_transform(predictions)\n",
        "    result = pd.DataFrame({'image': names, 'label': pred_labels})\n",
        "    result.to_csv('prediction.csv', index=False)\n",
        "\n",
        "    print(\"Hoàn thành!\")\n",
        "    print(f\"Đã lưu {len(predictions)} dự đoán vào prediction.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lA1Su6I3BOp",
        "outputId": "eb68146d-60ba-49bd-f395-d286ceb24a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thiết bị: cuda\n",
            "Đã tải 100/900 ảnh\n",
            "Đã tải 200/900 ảnh\n",
            "Đã tải 300/900 ảnh\n",
            "Đã tải 400/900 ảnh\n",
            "Đã tải 500/900 ảnh\n",
            "Đã tải 600/900 ảnh\n",
            "Đã tải 700/900 ảnh\n",
            "Đã tải 800/900 ảnh\n",
            "Đã tải 900/900 ảnh\n",
            "Tải thành công 900 ảnh\n",
            "Số lớp: 30\n",
            "Mean: [0.48047017 0.50314857 0.45571304], Std: [0.24075625 0.23772563 0.27127864]\n",
            "Bắt đầu huấn luyện...\n",
            "Epoch 1/50, Batch 0, Loss: 4.4891, Acc: 0.00%\n",
            "Epoch 1/50, Batch 10, Loss: 3.7841, Acc: 3.41%\n",
            "Epoch 1/50, Batch 20, Loss: 3.8434, Acc: 3.57%\n",
            "Epoch 1/50, Batch 30, Loss: 3.9349, Acc: 4.03%\n",
            "Epoch 1/50, Batch 40, Loss: 4.0278, Acc: 4.12%\n",
            "Epoch 1 - Train Loss: 3.8942, Train Acc: 4.72%, Val Loss: 3.5293, Val Acc: 8.89%\n",
            "Epoch 2/50, Batch 0, Loss: 3.6393, Acc: 0.00%\n",
            "Epoch 2/50, Batch 10, Loss: 3.5839, Acc: 3.41%\n",
            "Epoch 2/50, Batch 20, Loss: 3.5399, Acc: 3.87%\n",
            "Epoch 2/50, Batch 30, Loss: 3.2768, Acc: 4.64%\n",
            "Epoch 2/50, Batch 40, Loss: 3.6906, Acc: 5.34%\n",
            "Epoch 2 - Train Loss: 3.6024, Train Acc: 5.28%, Val Loss: 3.4332, Val Acc: 10.56%\n",
            "Epoch 3/50, Batch 0, Loss: 3.5495, Acc: 6.25%\n",
            "Epoch 3/50, Batch 10, Loss: 3.2325, Acc: 7.95%\n",
            "Epoch 3/50, Batch 20, Loss: 3.0842, Acc: 7.44%\n",
            "Epoch 3/50, Batch 30, Loss: 3.5222, Acc: 8.06%\n",
            "Epoch 3/50, Batch 40, Loss: 3.6331, Acc: 7.93%\n",
            "Epoch 3 - Train Loss: 3.4432, Train Acc: 8.61%, Val Loss: 3.2942, Val Acc: 10.00%\n",
            "Epoch 4/50, Batch 0, Loss: 3.5780, Acc: 6.25%\n",
            "Epoch 4/50, Batch 10, Loss: 3.6997, Acc: 10.23%\n",
            "Epoch 4/50, Batch 20, Loss: 3.4883, Acc: 10.12%\n",
            "Epoch 4/50, Batch 30, Loss: 3.1948, Acc: 9.68%\n",
            "Epoch 4/50, Batch 40, Loss: 3.0254, Acc: 9.15%\n",
            "Epoch 4 - Train Loss: 3.3836, Train Acc: 9.31%, Val Loss: 3.2119, Val Acc: 12.22%\n",
            "Epoch 5/50, Batch 0, Loss: 2.9681, Acc: 0.00%\n",
            "Epoch 5/50, Batch 10, Loss: 3.3328, Acc: 8.52%\n",
            "Epoch 5/50, Batch 20, Loss: 3.0210, Acc: 10.12%\n",
            "Epoch 5/50, Batch 30, Loss: 3.3398, Acc: 10.89%\n",
            "Epoch 5/50, Batch 40, Loss: 3.8196, Acc: 9.76%\n",
            "Epoch 5 - Train Loss: 3.2899, Train Acc: 10.14%, Val Loss: 3.2056, Val Acc: 13.33%\n",
            "Epoch 6/50, Batch 0, Loss: 3.2847, Acc: 12.50%\n",
            "Epoch 6/50, Batch 10, Loss: 3.1495, Acc: 13.64%\n",
            "Epoch 6/50, Batch 20, Loss: 3.1039, Acc: 13.39%\n",
            "Epoch 6/50, Batch 30, Loss: 3.2830, Acc: 11.69%\n",
            "Epoch 6/50, Batch 40, Loss: 3.1181, Acc: 11.89%\n",
            "Epoch 6 - Train Loss: 3.2492, Train Acc: 11.94%, Val Loss: 3.1619, Val Acc: 15.00%\n",
            "Epoch 7/50, Batch 0, Loss: 3.1709, Acc: 18.75%\n",
            "Epoch 7/50, Batch 10, Loss: 3.0124, Acc: 8.52%\n",
            "Epoch 7/50, Batch 20, Loss: 3.1386, Acc: 9.23%\n",
            "Epoch 7/50, Batch 30, Loss: 3.3144, Acc: 11.09%\n",
            "Epoch 7/50, Batch 40, Loss: 3.0345, Acc: 11.13%\n",
            "Epoch 7 - Train Loss: 3.2399, Train Acc: 11.11%, Val Loss: 3.0627, Val Acc: 16.11%\n",
            "Epoch 8/50, Batch 0, Loss: 3.0611, Acc: 12.50%\n",
            "Epoch 8/50, Batch 10, Loss: 3.1813, Acc: 14.77%\n",
            "Epoch 8/50, Batch 20, Loss: 3.1998, Acc: 14.58%\n",
            "Epoch 8/50, Batch 30, Loss: 3.1146, Acc: 13.91%\n",
            "Epoch 8/50, Batch 40, Loss: 3.2146, Acc: 13.87%\n",
            "Epoch 8 - Train Loss: 3.1774, Train Acc: 13.06%, Val Loss: 3.0714, Val Acc: 16.11%\n",
            "Epoch 9/50, Batch 0, Loss: 2.9435, Acc: 18.75%\n",
            "Epoch 9/50, Batch 10, Loss: 3.2038, Acc: 18.18%\n",
            "Epoch 9/50, Batch 20, Loss: 3.1598, Acc: 15.77%\n",
            "Epoch 9/50, Batch 30, Loss: 3.4661, Acc: 15.73%\n",
            "Epoch 9/50, Batch 40, Loss: 3.3185, Acc: 14.33%\n",
            "Epoch 9 - Train Loss: 3.1249, Train Acc: 14.58%, Val Loss: 3.0771, Val Acc: 19.44%\n",
            "Epoch 10/50, Batch 0, Loss: 3.2293, Acc: 12.50%\n",
            "Epoch 10/50, Batch 10, Loss: 2.9771, Acc: 15.34%\n",
            "Epoch 10/50, Batch 20, Loss: 3.1364, Acc: 15.18%\n",
            "Epoch 10/50, Batch 30, Loss: 3.3079, Acc: 14.31%\n",
            "Epoch 10/50, Batch 40, Loss: 3.0734, Acc: 12.96%\n",
            "Epoch 10 - Train Loss: 3.1180, Train Acc: 13.33%, Val Loss: 3.0378, Val Acc: 18.89%\n",
            "Epoch 11/50, Batch 0, Loss: 3.0030, Acc: 12.50%\n",
            "Epoch 11/50, Batch 10, Loss: 3.4813, Acc: 13.07%\n",
            "Epoch 11/50, Batch 20, Loss: 3.1578, Acc: 13.69%\n",
            "Epoch 11/50, Batch 30, Loss: 3.0052, Acc: 13.91%\n",
            "Epoch 11/50, Batch 40, Loss: 3.3423, Acc: 14.33%\n",
            "Epoch 11 - Train Loss: 3.0924, Train Acc: 14.31%, Val Loss: 3.0657, Val Acc: 17.78%\n",
            "Epoch 12/50, Batch 0, Loss: 2.8707, Acc: 25.00%\n",
            "Epoch 12/50, Batch 10, Loss: 3.1092, Acc: 12.50%\n",
            "Epoch 12/50, Batch 20, Loss: 2.9480, Acc: 12.80%\n",
            "Epoch 12/50, Batch 30, Loss: 2.8739, Acc: 12.50%\n",
            "Epoch 12/50, Batch 40, Loss: 3.0377, Acc: 12.96%\n",
            "Epoch 12 - Train Loss: 3.0807, Train Acc: 14.03%, Val Loss: 3.0135, Val Acc: 17.22%\n",
            "Epoch 13/50, Batch 0, Loss: 2.8216, Acc: 18.75%\n",
            "Epoch 13/50, Batch 10, Loss: 3.0539, Acc: 16.48%\n",
            "Epoch 13/50, Batch 20, Loss: 2.8128, Acc: 18.45%\n",
            "Epoch 13/50, Batch 30, Loss: 3.0645, Acc: 18.55%\n",
            "Epoch 13/50, Batch 40, Loss: 3.3000, Acc: 17.68%\n",
            "Epoch 13 - Train Loss: 3.0555, Train Acc: 17.36%, Val Loss: 3.0577, Val Acc: 20.56%\n",
            "Epoch 14/50, Batch 0, Loss: 3.0846, Acc: 6.25%\n",
            "Epoch 14/50, Batch 10, Loss: 3.1691, Acc: 13.64%\n",
            "Epoch 14/50, Batch 20, Loss: 2.9561, Acc: 15.77%\n",
            "Epoch 14/50, Batch 30, Loss: 3.1865, Acc: 15.52%\n",
            "Epoch 14/50, Batch 40, Loss: 3.0847, Acc: 14.63%\n",
            "Epoch 14 - Train Loss: 3.0876, Train Acc: 14.86%, Val Loss: 2.9907, Val Acc: 21.67%\n",
            "Epoch 15/50, Batch 0, Loss: 2.9562, Acc: 12.50%\n",
            "Epoch 15/50, Batch 10, Loss: 2.9661, Acc: 14.77%\n",
            "Epoch 15/50, Batch 20, Loss: 2.8421, Acc: 16.67%\n",
            "Epoch 15/50, Batch 30, Loss: 2.8352, Acc: 16.53%\n",
            "Epoch 15/50, Batch 40, Loss: 2.8856, Acc: 16.31%\n",
            "Epoch 15 - Train Loss: 3.0115, Train Acc: 17.08%, Val Loss: 3.0157, Val Acc: 18.33%\n",
            "Epoch 16/50, Batch 0, Loss: 2.8019, Acc: 18.75%\n",
            "Epoch 16/50, Batch 10, Loss: 2.9472, Acc: 22.73%\n",
            "Epoch 16/50, Batch 20, Loss: 3.0237, Acc: 17.26%\n",
            "Epoch 16/50, Batch 30, Loss: 2.7711, Acc: 18.75%\n",
            "Epoch 16/50, Batch 40, Loss: 2.9437, Acc: 17.38%\n",
            "Epoch 16 - Train Loss: 3.0353, Train Acc: 17.08%, Val Loss: 2.9608, Val Acc: 21.67%\n",
            "Epoch 17/50, Batch 0, Loss: 3.3525, Acc: 0.00%\n",
            "Epoch 17/50, Batch 10, Loss: 2.9526, Acc: 15.34%\n",
            "Epoch 17/50, Batch 20, Loss: 2.8980, Acc: 16.07%\n",
            "Epoch 17/50, Batch 30, Loss: 3.3753, Acc: 16.53%\n",
            "Epoch 17/50, Batch 40, Loss: 2.9604, Acc: 16.62%\n",
            "Epoch 17 - Train Loss: 3.0390, Train Acc: 16.67%, Val Loss: 2.9703, Val Acc: 19.44%\n",
            "Epoch 18/50, Batch 0, Loss: 3.0177, Acc: 18.75%\n",
            "Epoch 18/50, Batch 10, Loss: 3.1233, Acc: 17.05%\n",
            "Epoch 18/50, Batch 20, Loss: 3.2371, Acc: 17.26%\n",
            "Epoch 18/50, Batch 30, Loss: 2.8149, Acc: 16.73%\n",
            "Epoch 18/50, Batch 40, Loss: 2.9524, Acc: 16.62%\n",
            "Epoch 18 - Train Loss: 3.0048, Train Acc: 16.39%, Val Loss: 2.9510, Val Acc: 20.00%\n",
            "Epoch 19/50, Batch 0, Loss: 3.1105, Acc: 12.50%\n",
            "Epoch 19/50, Batch 10, Loss: 2.7768, Acc: 18.75%\n",
            "Epoch 19/50, Batch 20, Loss: 2.7955, Acc: 18.75%\n",
            "Epoch 19/50, Batch 30, Loss: 2.9624, Acc: 19.76%\n",
            "Epoch 19/50, Batch 40, Loss: 2.7912, Acc: 18.45%\n",
            "Epoch 19 - Train Loss: 2.9616, Train Acc: 19.17%, Val Loss: 3.0234, Val Acc: 20.56%\n",
            "Epoch 20/50, Batch 0, Loss: 2.9495, Acc: 18.75%\n",
            "Epoch 20/50, Batch 10, Loss: 2.5669, Acc: 21.59%\n",
            "Epoch 20/50, Batch 20, Loss: 2.8964, Acc: 20.83%\n",
            "Epoch 20/50, Batch 30, Loss: 2.9478, Acc: 19.15%\n",
            "Epoch 20/50, Batch 40, Loss: 3.2033, Acc: 17.99%\n",
            "Epoch 20 - Train Loss: 2.9295, Train Acc: 18.33%, Val Loss: 2.9757, Val Acc: 21.67%\n",
            "Epoch 21/50, Batch 0, Loss: 2.8993, Acc: 18.75%\n",
            "Epoch 21/50, Batch 10, Loss: 3.2740, Acc: 21.02%\n",
            "Epoch 21/50, Batch 20, Loss: 2.7331, Acc: 23.51%\n",
            "Epoch 21/50, Batch 30, Loss: 2.8743, Acc: 22.18%\n",
            "Epoch 21/50, Batch 40, Loss: 2.8529, Acc: 21.65%\n",
            "Epoch 21 - Train Loss: 2.9232, Train Acc: 20.69%, Val Loss: 2.9887, Val Acc: 21.11%\n",
            "Epoch 22/50, Batch 0, Loss: 2.8089, Acc: 18.75%\n",
            "Epoch 22/50, Batch 10, Loss: 2.6413, Acc: 25.00%\n",
            "Epoch 22/50, Batch 20, Loss: 3.0050, Acc: 23.81%\n",
            "Epoch 22/50, Batch 30, Loss: 2.9837, Acc: 21.77%\n",
            "Epoch 22/50, Batch 40, Loss: 2.7896, Acc: 20.58%\n",
            "Epoch 22 - Train Loss: 2.9045, Train Acc: 20.56%, Val Loss: 2.9617, Val Acc: 19.44%\n",
            "Epoch 23/50, Batch 0, Loss: 2.6741, Acc: 31.25%\n",
            "Epoch 23/50, Batch 10, Loss: 2.8865, Acc: 19.32%\n",
            "Epoch 23/50, Batch 20, Loss: 2.5506, Acc: 18.45%\n",
            "Epoch 23/50, Batch 30, Loss: 2.6254, Acc: 17.14%\n",
            "Epoch 23/50, Batch 40, Loss: 2.4228, Acc: 17.99%\n",
            "Epoch 23 - Train Loss: 2.9336, Train Acc: 18.47%, Val Loss: 2.9674, Val Acc: 21.67%\n",
            "Epoch 24/50, Batch 0, Loss: 3.0900, Acc: 31.25%\n",
            "Epoch 24/50, Batch 10, Loss: 2.8576, Acc: 22.73%\n",
            "Epoch 24/50, Batch 20, Loss: 2.7753, Acc: 20.83%\n",
            "Epoch 24/50, Batch 30, Loss: 3.1944, Acc: 19.96%\n",
            "Epoch 24/50, Batch 40, Loss: 3.4612, Acc: 18.90%\n",
            "Epoch 24 - Train Loss: 2.9284, Train Acc: 18.33%, Val Loss: 2.9352, Val Acc: 21.67%\n",
            "Epoch 25/50, Batch 0, Loss: 2.4003, Acc: 37.50%\n",
            "Epoch 25/50, Batch 10, Loss: 2.4461, Acc: 22.73%\n",
            "Epoch 25/50, Batch 20, Loss: 3.0745, Acc: 20.54%\n",
            "Epoch 25/50, Batch 30, Loss: 3.6486, Acc: 21.37%\n",
            "Epoch 25/50, Batch 40, Loss: 2.9006, Acc: 21.49%\n",
            "Epoch 25 - Train Loss: 2.9181, Train Acc: 21.25%, Val Loss: 2.9446, Val Acc: 21.11%\n",
            "Epoch 26/50, Batch 0, Loss: 2.7304, Acc: 25.00%\n",
            "Epoch 26/50, Batch 10, Loss: 2.6766, Acc: 21.59%\n",
            "Epoch 26/50, Batch 20, Loss: 2.7262, Acc: 19.05%\n",
            "Epoch 26/50, Batch 30, Loss: 3.1543, Acc: 19.15%\n",
            "Epoch 26/50, Batch 40, Loss: 2.7862, Acc: 18.75%\n",
            "Epoch 26 - Train Loss: 2.9128, Train Acc: 18.89%, Val Loss: 2.9340, Val Acc: 21.67%\n",
            "Epoch 27/50, Batch 0, Loss: 2.9361, Acc: 31.25%\n",
            "Epoch 27/50, Batch 10, Loss: 2.9886, Acc: 25.00%\n",
            "Epoch 27/50, Batch 20, Loss: 3.1523, Acc: 23.81%\n",
            "Epoch 27/50, Batch 30, Loss: 2.9113, Acc: 22.18%\n",
            "Epoch 27/50, Batch 40, Loss: 2.9767, Acc: 21.80%\n",
            "Epoch 27 - Train Loss: 2.8472, Train Acc: 22.08%, Val Loss: 2.9793, Val Acc: 20.56%\n",
            "Epoch 28/50, Batch 0, Loss: 3.1851, Acc: 18.75%\n",
            "Epoch 28/50, Batch 10, Loss: 2.9944, Acc: 21.02%\n",
            "Epoch 28/50, Batch 20, Loss: 3.5800, Acc: 20.54%\n",
            "Epoch 28/50, Batch 30, Loss: 3.1113, Acc: 21.77%\n",
            "Epoch 28/50, Batch 40, Loss: 2.8923, Acc: 21.80%\n",
            "Epoch 28 - Train Loss: 2.9022, Train Acc: 22.08%, Val Loss: 2.8970, Val Acc: 19.44%\n",
            "Epoch 29/50, Batch 0, Loss: 2.8236, Acc: 25.00%\n",
            "Epoch 29/50, Batch 10, Loss: 2.8249, Acc: 22.16%\n",
            "Epoch 29/50, Batch 20, Loss: 2.8183, Acc: 22.62%\n",
            "Epoch 29/50, Batch 30, Loss: 2.5436, Acc: 23.39%\n",
            "Epoch 29/50, Batch 40, Loss: 2.4923, Acc: 22.41%\n",
            "Epoch 29 - Train Loss: 2.8833, Train Acc: 21.53%, Val Loss: 2.8952, Val Acc: 21.67%\n",
            "Epoch 30/50, Batch 0, Loss: 2.7944, Acc: 25.00%\n",
            "Epoch 30/50, Batch 10, Loss: 2.8152, Acc: 23.86%\n",
            "Epoch 30/50, Batch 20, Loss: 2.7642, Acc: 24.70%\n",
            "Epoch 30/50, Batch 30, Loss: 3.0175, Acc: 24.19%\n",
            "Epoch 30/50, Batch 40, Loss: 3.2195, Acc: 24.39%\n",
            "Epoch 30 - Train Loss: 2.8496, Train Acc: 24.44%, Val Loss: 2.9403, Val Acc: 21.67%\n",
            "Epoch 31/50, Batch 0, Loss: 2.7510, Acc: 18.75%\n",
            "Epoch 31/50, Batch 10, Loss: 2.5171, Acc: 26.70%\n",
            "Epoch 31/50, Batch 20, Loss: 3.1562, Acc: 22.02%\n",
            "Epoch 31/50, Batch 30, Loss: 2.6546, Acc: 22.38%\n",
            "Epoch 31/50, Batch 40, Loss: 2.8961, Acc: 21.80%\n",
            "Epoch 31 - Train Loss: 2.8362, Train Acc: 21.11%, Val Loss: 2.9526, Val Acc: 21.11%\n",
            "Epoch 32/50, Batch 0, Loss: 2.7314, Acc: 25.00%\n",
            "Epoch 32/50, Batch 10, Loss: 3.1371, Acc: 20.45%\n",
            "Epoch 32/50, Batch 20, Loss: 2.6471, Acc: 22.92%\n",
            "Epoch 32/50, Batch 30, Loss: 2.7864, Acc: 22.38%\n",
            "Epoch 32/50, Batch 40, Loss: 3.0938, Acc: 20.73%\n",
            "Epoch 32 - Train Loss: 2.8085, Train Acc: 22.08%, Val Loss: 2.8929, Val Acc: 22.22%\n",
            "Epoch 33/50, Batch 0, Loss: 3.2079, Acc: 6.25%\n",
            "Epoch 33/50, Batch 10, Loss: 2.9579, Acc: 27.27%\n",
            "Epoch 33/50, Batch 20, Loss: 2.7792, Acc: 22.32%\n",
            "Epoch 33/50, Batch 30, Loss: 2.9028, Acc: 22.58%\n",
            "Epoch 33/50, Batch 40, Loss: 2.5799, Acc: 21.34%\n",
            "Epoch 33 - Train Loss: 2.8276, Train Acc: 21.94%, Val Loss: 2.9345, Val Acc: 21.11%\n",
            "Epoch 34/50, Batch 0, Loss: 2.7105, Acc: 18.75%\n",
            "Epoch 34/50, Batch 10, Loss: 2.8829, Acc: 25.00%\n",
            "Epoch 34/50, Batch 20, Loss: 3.0562, Acc: 26.49%\n",
            "Epoch 34/50, Batch 30, Loss: 2.6469, Acc: 23.19%\n",
            "Epoch 34/50, Batch 40, Loss: 2.4160, Acc: 22.87%\n",
            "Epoch 34 - Train Loss: 2.8192, Train Acc: 22.50%, Val Loss: 2.9529, Val Acc: 21.11%\n",
            "Epoch 35/50, Batch 0, Loss: 2.9165, Acc: 25.00%\n",
            "Epoch 35/50, Batch 10, Loss: 3.0440, Acc: 22.16%\n",
            "Epoch 35/50, Batch 20, Loss: 2.8848, Acc: 23.21%\n",
            "Epoch 35/50, Batch 30, Loss: 2.6726, Acc: 22.98%\n",
            "Epoch 35/50, Batch 40, Loss: 2.7025, Acc: 21.95%\n",
            "Epoch 35 - Train Loss: 2.8022, Train Acc: 22.22%, Val Loss: 2.9434, Val Acc: 21.67%\n",
            "Epoch 36/50, Batch 0, Loss: 3.1251, Acc: 6.25%\n",
            "Epoch 36/50, Batch 10, Loss: 2.4868, Acc: 17.05%\n",
            "Epoch 36/50, Batch 20, Loss: 2.5695, Acc: 20.54%\n",
            "Epoch 36/50, Batch 30, Loss: 3.1768, Acc: 23.19%\n",
            "Epoch 36/50, Batch 40, Loss: 2.8307, Acc: 22.56%\n",
            "Epoch 36 - Train Loss: 2.8343, Train Acc: 22.64%, Val Loss: 2.9171, Val Acc: 21.11%\n",
            "Epoch 37/50, Batch 0, Loss: 2.6018, Acc: 31.25%\n",
            "Epoch 37/50, Batch 10, Loss: 3.0589, Acc: 26.14%\n",
            "Epoch 37/50, Batch 20, Loss: 2.4802, Acc: 25.30%\n",
            "Epoch 37/50, Batch 30, Loss: 2.7155, Acc: 25.00%\n",
            "Epoch 37/50, Batch 40, Loss: 2.8758, Acc: 24.85%\n",
            "Epoch 37 - Train Loss: 2.7965, Train Acc: 25.42%, Val Loss: 2.9285, Val Acc: 21.11%\n",
            "Epoch 38/50, Batch 0, Loss: 3.2612, Acc: 6.25%\n",
            "Epoch 38/50, Batch 10, Loss: 3.0792, Acc: 31.25%\n",
            "Epoch 38/50, Batch 20, Loss: 2.8826, Acc: 25.00%\n",
            "Epoch 38/50, Batch 30, Loss: 2.8998, Acc: 25.00%\n",
            "Epoch 38/50, Batch 40, Loss: 2.6072, Acc: 26.22%\n",
            "Epoch 38 - Train Loss: 2.7774, Train Acc: 25.14%, Val Loss: 2.9107, Val Acc: 21.67%\n",
            "Epoch 39/50, Batch 0, Loss: 3.2218, Acc: 6.25%\n",
            "Epoch 39/50, Batch 10, Loss: 3.3141, Acc: 27.27%\n",
            "Epoch 39/50, Batch 20, Loss: 2.4885, Acc: 25.60%\n",
            "Epoch 39/50, Batch 30, Loss: 2.8233, Acc: 24.60%\n",
            "Epoch 39/50, Batch 40, Loss: 3.1208, Acc: 24.85%\n",
            "Epoch 39 - Train Loss: 2.7875, Train Acc: 24.86%, Val Loss: 2.8905, Val Acc: 22.22%\n",
            "Epoch 40/50, Batch 0, Loss: 2.6867, Acc: 25.00%\n",
            "Epoch 40/50, Batch 10, Loss: 2.9158, Acc: 25.00%\n",
            "Epoch 40/50, Batch 20, Loss: 2.4808, Acc: 26.79%\n",
            "Epoch 40/50, Batch 30, Loss: 3.0625, Acc: 25.40%\n",
            "Epoch 40/50, Batch 40, Loss: 2.8297, Acc: 26.07%\n",
            "Epoch 40 - Train Loss: 2.7861, Train Acc: 25.42%, Val Loss: 2.8772, Val Acc: 21.67%\n",
            "Epoch 41/50, Batch 0, Loss: 2.4710, Acc: 43.75%\n",
            "Epoch 41/50, Batch 10, Loss: 2.8961, Acc: 22.73%\n",
            "Epoch 41/50, Batch 20, Loss: 2.6186, Acc: 25.00%\n",
            "Epoch 41/50, Batch 30, Loss: 2.8287, Acc: 24.60%\n",
            "Epoch 41/50, Batch 40, Loss: 2.9627, Acc: 23.63%\n",
            "Epoch 41 - Train Loss: 2.7801, Train Acc: 24.44%, Val Loss: 2.8897, Val Acc: 21.11%\n",
            "Epoch 42/50, Batch 0, Loss: 2.5355, Acc: 31.25%\n",
            "Epoch 42/50, Batch 10, Loss: 2.9197, Acc: 19.89%\n",
            "Epoch 42/50, Batch 20, Loss: 2.7026, Acc: 22.02%\n",
            "Epoch 42/50, Batch 30, Loss: 3.0487, Acc: 24.60%\n",
            "Epoch 42/50, Batch 40, Loss: 2.5981, Acc: 24.85%\n",
            "Epoch 42 - Train Loss: 2.7505, Train Acc: 24.58%, Val Loss: 2.9059, Val Acc: 21.67%\n",
            "Epoch 43/50, Batch 0, Loss: 2.8392, Acc: 12.50%\n",
            "Epoch 43/50, Batch 10, Loss: 2.7598, Acc: 23.86%\n",
            "Epoch 43/50, Batch 20, Loss: 2.8402, Acc: 23.21%\n",
            "Epoch 43/50, Batch 30, Loss: 2.6778, Acc: 25.20%\n",
            "Epoch 43/50, Batch 40, Loss: 2.5733, Acc: 24.39%\n",
            "Epoch 43 - Train Loss: 2.7687, Train Acc: 24.72%, Val Loss: 2.8850, Val Acc: 21.67%\n",
            "Epoch 44/50, Batch 0, Loss: 2.8864, Acc: 25.00%\n",
            "Epoch 44/50, Batch 10, Loss: 3.4210, Acc: 25.00%\n",
            "Epoch 44/50, Batch 20, Loss: 2.6610, Acc: 25.60%\n",
            "Epoch 44/50, Batch 30, Loss: 2.5430, Acc: 25.00%\n",
            "Epoch 44/50, Batch 40, Loss: 2.5394, Acc: 24.85%\n",
            "Epoch 44 - Train Loss: 2.6946, Train Acc: 25.28%, Val Loss: 2.9110, Val Acc: 22.78%\n",
            "Epoch 45/50, Batch 0, Loss: 2.3422, Acc: 25.00%\n",
            "Epoch 45/50, Batch 10, Loss: 2.5733, Acc: 21.59%\n",
            "Epoch 45/50, Batch 20, Loss: 2.7065, Acc: 24.40%\n",
            "Epoch 45/50, Batch 30, Loss: 3.1072, Acc: 23.19%\n",
            "Epoch 45/50, Batch 40, Loss: 2.9256, Acc: 24.24%\n",
            "Epoch 45 - Train Loss: 2.7126, Train Acc: 24.58%, Val Loss: 2.9068, Val Acc: 20.56%\n",
            "Epoch 46/50, Batch 0, Loss: 2.4449, Acc: 37.50%\n",
            "Epoch 46/50, Batch 10, Loss: 2.8443, Acc: 21.02%\n",
            "Epoch 46/50, Batch 20, Loss: 2.4836, Acc: 26.49%\n",
            "Epoch 46/50, Batch 30, Loss: 2.6523, Acc: 26.01%\n",
            "Epoch 46/50, Batch 40, Loss: 2.6453, Acc: 26.68%\n",
            "Epoch 46 - Train Loss: 2.6942, Train Acc: 27.08%, Val Loss: 2.9015, Val Acc: 21.67%\n",
            "Epoch 47/50, Batch 0, Loss: 2.5028, Acc: 31.25%\n",
            "Epoch 47/50, Batch 10, Loss: 2.8845, Acc: 23.30%\n",
            "Epoch 47/50, Batch 20, Loss: 2.9956, Acc: 23.21%\n",
            "Epoch 47/50, Batch 30, Loss: 2.2844, Acc: 24.40%\n",
            "Epoch 47/50, Batch 40, Loss: 2.4638, Acc: 23.93%\n",
            "Epoch 47 - Train Loss: 2.7275, Train Acc: 23.61%, Val Loss: 2.9068, Val Acc: 22.78%\n",
            "Epoch 48/50, Batch 0, Loss: 2.9369, Acc: 25.00%\n",
            "Epoch 48/50, Batch 10, Loss: 2.4663, Acc: 27.27%\n",
            "Epoch 48/50, Batch 20, Loss: 2.6870, Acc: 25.00%\n",
            "Epoch 48/50, Batch 30, Loss: 2.7614, Acc: 24.60%\n",
            "Epoch 48/50, Batch 40, Loss: 2.4804, Acc: 25.61%\n",
            "Epoch 48 - Train Loss: 2.7448, Train Acc: 25.56%, Val Loss: 2.8982, Val Acc: 21.67%\n",
            "Epoch 49/50, Batch 0, Loss: 2.9135, Acc: 25.00%\n",
            "Epoch 49/50, Batch 10, Loss: 2.7970, Acc: 25.57%\n",
            "Epoch 49/50, Batch 20, Loss: 2.7383, Acc: 27.68%\n",
            "Epoch 49/50, Batch 30, Loss: 2.6165, Acc: 27.02%\n",
            "Epoch 49/50, Batch 40, Loss: 2.5859, Acc: 27.13%\n",
            "Epoch 49 - Train Loss: 2.7138, Train Acc: 27.50%, Val Loss: 2.9011, Val Acc: 21.11%\n",
            "Epoch 50/50, Batch 0, Loss: 2.7340, Acc: 25.00%\n",
            "Epoch 50/50, Batch 10, Loss: 2.8697, Acc: 21.02%\n",
            "Epoch 50/50, Batch 20, Loss: 2.4896, Acc: 23.51%\n",
            "Epoch 50/50, Batch 30, Loss: 2.7870, Acc: 22.98%\n",
            "Epoch 50/50, Batch 40, Loss: 2.8654, Acc: 23.93%\n",
            "Epoch 50 - Train Loss: 2.7523, Train Acc: 24.03%, Val Loss: 2.9139, Val Acc: 20.56%\n",
            "Đã tải 100/801 ảnh\n",
            "Đã tải 200/801 ảnh\n",
            "Đã tải 300/801 ảnh\n",
            "Đã tải 400/801 ảnh\n",
            "Đã tải 500/801 ảnh\n",
            "Đã tải 600/801 ảnh\n",
            "Đã tải 700/801 ảnh\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}